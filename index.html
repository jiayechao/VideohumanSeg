<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Document</title>
  <style lang="less" scoped>
    body {
      margin: 0 auto;
      width: 100%;
      height: 100%;
      overflow: auto;
    }
    .container {
      position: relative;
      margin-top: 60px;
    }
    video {
      width: 640px;
      object-fit: contain;
    }
    #video-tool {
      display: flex;
    }
    .bg-list {
      display: flex;
    }
    .bg {
      width: 80px;
      height: 80px;
      border-radius: 50%;
      cursor: pointer;
      margin-right: 20px;
    }
    #demo {
      display: none;
    }
    </style>
</head>
<body>
  <button id="start" type="btn">开始</button>
  <div class="container">
    <video id="video" autoplay></video>
    <canvas id="demo"></canvas>
  </div>
  <div class="bg-list">
    <div class="bg" onclick="bgImg = null" style="line-height: 80px;background-color: #000;text-align: center;color: #fff;"  id="bg0">无背景</div>
    <img class="bg" onclick="bgImg = this" id="bg1" src="./bgImgs/bg1.png"/>
    <img class="bg" onclick="bgImg = this"id="bg2" src="./bgImgs/bg2.jpg"/>
    <img class="bg" onclick="bgImg = this"id="bg3" src="./bgImgs/bg3.jpg"/>
    <img class="bg" onclick="bgImg = this"id="bg4" src="./bgImgs/bg4.jpg"/>
  </div>
  
  
  <!-- 加载sdk -->
  <script src="/VideohumanSeg/dist/humanSegGpu.js"></script>
  
<script>
var bgImg
async function load() {
  await humanSegGpu.load('/VideohumanSeg/models/model.json')
}
load()

window.onload = async function() {
  const video = document.getElementById('video')
  const canvas1 = document.getElementById('demo')
  const startBtn = document.getElementById('start')

  const videoCanvas = document.createElement('canvas')
  const videoCanvasCtx = videoCanvas.getContext('2d')

  // 点击视频控制按钮，实现视频播放/截图/暂停功能
  startBtn.addEventListener('click', function (e) {
    // 请求摄像头权限
    navigator.getUserMedia(
      { video: true },
      function (stream) {
        let [track] = stream.getVideoTracks()
          const segmentGenerator = new MediaStreamTrackGenerator({
            kind: 'video',
          })
          const processor = new MediaStreamTrackProcessor({ track })
          const segmentStream = new MediaStream([segmentGenerator])

          video.srcObject = segmentStream

          processor.readable
            .pipeThrough(
              new TransformStream({
                transform: (frame, controller) => step(frame, controller),
              }),
            )
            .pipeTo(segmentGenerator.writable)
            .catch((err) => console.error('green screen generator error', err))
      },
      function (err) {
        console.log(err)
      },
    )
  })

  async function step(frame, controller) {
    let selfieFrame
    if(bgImg) {
      const width = frame.codedWidth
      const height = frame.codedHeight
      videoCanvas.width = width
      videoCanvas.height = height
      videoCanvasCtx.drawImage(frame, 0, 0, width, height)
      await humanSegGpu.drawHumanSeg(videoCanvas, canvas1, bgImg)
      selfieFrame = new VideoFrame(canvas1, { timestamp: 1 })
    }
    controller.enqueue(bgImg ? selfieFrame : frame)
    frame.close()
  }
  
  

  
  function drawCanvas(canvas, img) {
    canvas.width = getComputedStyle(canvas).width.split('px')[0];
    canvas.height = getComputedStyle(canvas).height.split('px')[0];
    let ratio  = Math.min(canvas.width / img.width, canvas.height / img.height);
    let x = (canvas.width - img.width * ratio) / 2;
    let y = (canvas.height - img.height * ratio) / 2;
    canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height);
    canvas.getContext('2d').drawImage(img, 0, 0, img.width, img.height,
        x, y, img.width * ratio, img.height * ratio);
  }

  
}
</script>
</body>
</html>